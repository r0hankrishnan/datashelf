{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataShelf Example\n",
    "\n",
    "This notebook demonstrates the core functionality of DataShelf - a git-like version control system for datasets.\n",
    "\n",
    "We'll walk through:\n",
    "1. Setting up a DataShelf project\n",
    "2. Creating collections to organize datasets\n",
    "3. Saving and versioning datasets\n",
    "4. Understanding the metadata structure\n",
    "\n",
    "*Thanks Claude for helping with the dialogue :)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and create some sample data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sales data:\n",
      "    product  units_sold  price region\n",
      "0  Widget A         150  25.99  North\n",
      "1  Widget B         200  45.50  South\n",
      "2  Widget C          75  15.00   East\n",
      "3  Widget D         300  60.00   West\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datashelf.core import init, create_collection, save\n",
    "\n",
    "# Create sample datasets for our example\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample sales data\n",
    "sales_data = pd.DataFrame({\n",
    "    'product': ['Widget A', 'Widget B', 'Widget C', 'Widget D'],\n",
    "    'units_sold': [150, 200, 75, 300],\n",
    "    'price': [25.99, 45.50, 15.00, 60.00],\n",
    "    'region': ['North', 'South', 'East', 'West']\n",
    "})\n",
    "\n",
    "print(\"Sample sales data:\")\n",
    "print(sales_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize DataShelf\n",
    "\n",
    "Before we can start versioning datasets, we need to initialize DataShelf in our project directory. This creates a `.datashelf` folder that will store all our metadata and dataset versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 20:32:44,504 - INFO - .datashelf directory and metadata initialized\n"
     ]
    }
   ],
   "source": [
    "# Note: This will prompt you to confirm the directory\n",
    "# In a real scenario, make sure you're in your project root\n",
    "init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Collection\n",
    "\n",
    "Collections help organize related datasets. Think of them like folders for your data versions. Let's create a collection for our sales analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 20:32:44,515 - INFO - collection directory: Sales Analysis 2024 and metadata file sales_analysis_2024_metadata.yaml created.\n"
     ]
    }
   ],
   "source": [
    "# Create a collection for sales data\n",
    "create_collection(\"Sales Analysis 2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save Dataset Versions\n",
    "\n",
    "Now we can start saving dataset versions. Each save includes:\n",
    "- **name**: A descriptive name for this dataset\n",
    "- **tag**: A label (like \"raw\", \"cleaned\", \"final\")\n",
    "- **message**: A commit message explaining what this version contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 20:32:44,529 - INFO - quarterly_sales added to Sales Analysis 2024\n"
     ]
    }
   ],
   "source": [
    "# Save the raw sales data\n",
    "save(df=sales_data, \n",
    "     collection_name=\"Sales Analysis 2024\", \n",
    "     name=\"quarterly_sales\", \n",
    "     tag=\"raw\", \n",
    "     message=\"Initial quarterly sales data from database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Creating Data Transformations\n",
    "\n",
    "Let's create some transformed versions of our data and save them as new versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched sales data:\n",
      "    product  units_sold  price region  revenue  revenue_per_unit\n",
      "0  Widget A         150  25.99  North   3898.5             25.99\n",
      "1  Widget B         200  45.50  South   9100.0             45.50\n",
      "2  Widget C          75  15.00   East   1125.0             15.00\n",
      "3  Widget D         300  60.00   West  18000.0             60.00\n"
     ]
    }
   ],
   "source": [
    "# Add calculated columns\n",
    "sales_enriched = sales_data.copy()\n",
    "sales_enriched['revenue'] = sales_enriched['units_sold'] * sales_enriched['price']\n",
    "sales_enriched['revenue_per_unit'] = sales_enriched['revenue'] / sales_enriched['units_sold']\n",
    "\n",
    "print(\"Enriched sales data:\")\n",
    "print(sales_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 20:32:44,568 - INFO - quarterly_sales added to Sales Analysis 2024\n"
     ]
    }
   ],
   "source": [
    "# Save the enriched version\n",
    "save(df=sales_enriched, \n",
    "     collection_name=\"Sales Analysis 2024\", \n",
    "     name=\"quarterly_sales\", \n",
    "     tag=\"enriched\", \n",
    "     message=\"Added revenue calculations and per-unit metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales summary:\n",
      "   total_units  total_revenue  avg_price top_product\n",
      "0          725        32123.5    36.6225    Widget D\n"
     ]
    }
   ],
   "source": [
    "# Create a summary dataset\n",
    "sales_summary = pd.DataFrame({\n",
    "    'total_units': [sales_enriched['units_sold'].sum()],\n",
    "    'total_revenue': [sales_enriched['revenue'].sum()],\n",
    "    'avg_price': [sales_enriched['price'].mean()],\n",
    "    'top_product': [sales_enriched.loc[sales_enriched['revenue'].idxmax(), 'product']]\n",
    "})\n",
    "\n",
    "print(\"Sales summary:\")\n",
    "print(sales_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 20:32:44,602 - INFO - sales_summary added to Sales Analysis 2024\n"
     ]
    }
   ],
   "source": [
    "# Save the summary\n",
    "save(df=sales_summary, \n",
    "     collection_name=\"Sales Analysis 2024\", \n",
    "     name=\"sales_summary\", \n",
    "     tag=\"final\", \n",
    "     message=\"Final summary statistics for quarterly report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Testing Duplicate Detection\n",
    "\n",
    "DataShelf automatically detects when you try to save identical data and prevents duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 20:32:44,612 - INFO - duplicate_test's hash matches a dataframe that is already saved in datashelf: quarterly_sales.\n"
     ]
    }
   ],
   "source": [
    "# Try to save the same data again\n",
    "save(df=sales_data, \n",
    "     collection_name=\"Sales Analysis 2024\", \n",
    "     name=\"duplicate_test\", \n",
    "     tag=\"raw\", \n",
    "     message=\"This should be detected as a duplicate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Working with Multiple Collections\n",
    "\n",
    "Let's create another collection to demonstrate organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 20:32:44,620 - INFO - collection directory: Customer Analytics and metadata file customer_analytics_metadata.yaml created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer data:\n",
      "   customer_id           name  total_purchases  avg_order_value\n",
      "0         1001  Alice Johnson                5             45.2\n",
      "1         1002      Bob Smith               12             67.8\n",
      "2         1003    Carol Davis                3             23.5\n",
      "3         1004   David Wilson                8             55.1\n",
      "4         1005      Eva Brown               15             89.9\n"
     ]
    }
   ],
   "source": [
    "# Create a second collection\n",
    "create_collection(\"Customer Analytics\")\n",
    "\n",
    "# Create some customer data\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': range(1001, 1006),\n",
    "    'name': ['Alice Johnson', 'Bob Smith', 'Carol Davis', 'David Wilson', 'Eva Brown'],\n",
    "    'total_purchases': [5, 12, 3, 8, 15],\n",
    "    'avg_order_value': [45.20, 67.80, 23.50, 55.10, 89.90]\n",
    "})\n",
    "\n",
    "print(\"Customer data:\")\n",
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 20:32:44,659 - INFO - customer_profiles added to Customer Analytics\n"
     ]
    }
   ],
   "source": [
    "# Save to the customer analytics collection\n",
    "save(df=customer_data, \n",
    "     collection_name=\"Customer Analytics\", \n",
    "     name=\"customer_profiles\", \n",
    "     tag=\"raw\", \n",
    "     message=\"Initial customer profile data from CRM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the DataShelf Structure\n",
    "\n",
    "Let's explore what DataShelf has created for us behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataShelf directory structure:\n",
      ".datashelf/\n",
      "├── customer_analytics\n",
      "│   ├── customer_analytics_metadata.yaml\n",
      "│   └── customer_profiles_raw.csv\n",
      "├── datashelf_metadata.yaml\n",
      "└── sales_analysis_2024\n",
      "    ├── quarterly_sales_enriched.csv\n",
      "    ├── quarterly_sales_raw.csv\n",
      "    ├── sales_analysis_2024_metadata.yaml\n",
      "    └── sales_summary_final.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Check the .datashelf directory structure\n",
    "def show_directory_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    items = sorted(os.listdir(path))\n",
    "    for i, item in enumerate(items):\n",
    "        item_path = os.path.join(path, item)\n",
    "        is_last = i == len(items) - 1\n",
    "        \n",
    "        current_prefix = \"└── \" if is_last else \"├── \"\n",
    "        print(f\"{prefix}{current_prefix}{item}\")\n",
    "        \n",
    "        if os.path.isdir(item_path) and not item.startswith('.'):\n",
    "            next_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "            show_directory_tree(item_path, next_prefix, max_depth, current_depth + 1)\n",
    "\n",
    "print(\"DataShelf directory structure:\")\n",
    "print(\".datashelf/\")\n",
    "show_directory_tree(\".datashelf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main DataShelf metadata:\n",
      "config:\n",
      "- date_created: '2025-07-21 20:32:44'\n",
      "  number_of_collections: 2\n",
      "  collections:\n",
      "  - sales_analysis_2024\n",
      "  - customer_analytics\n",
      "collections:\n",
      "- collection_name: sales_analysis_2024\n",
      "  date_created: '2025-07-21 20:32:44'\n",
      "  date_last_modified: '2025-07-21 20:32:44'\n",
      "  files:\n",
      "  - sales_analysis_2024_metadata.yaml\n",
      "  - quarterly_sales\n",
      "  - quarterly_sales\n",
      "  - sales_summary\n",
      "- collection_name: customer_analytics\n",
      "  date_created: '2025-07-21 20:32:44'\n",
      "  date_last_modified: '2025-07-21 20:32:44'\n",
      "  files:\n",
      "  - customer_analytics_metadata.yaml\n",
      "  - customer_profiles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the main metadata file\n",
    "with open('.datashelf/datashelf_metadata.yaml', 'r') as f:\n",
    "    main_metadata = yaml.safe_load(f)\n",
    "\n",
    "print(\"Main DataShelf metadata:\")\n",
    "print(yaml.dump(main_metadata, default_flow_style=False, sort_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Analysis 2024 collection metadata:\n",
      "config:\n",
      "- collection_name: sales_analysis_2024\n",
      "  date_created: '2025-07-21 20:32:44'\n",
      "  number_of_files: 4\n",
      "  most_recent_commit: /Users/rohankrishnan/Documents/GitHub/datashelf/examples/.datashelf/sales_analysis_2024/sales_summary_final.csv\n",
      "  date_last_modified: '2025-07-21 20:32:44'\n",
      "files:\n",
      "- name: sales_analysis_2024_metadata.yaml\n",
      "  hash: ''\n",
      "  date_created: '2025-07-21 20:32:44'\n",
      "  date_last_modified: ''\n",
      "  tag: ''\n",
      "  version: null\n",
      "  message: ''\n",
      "- name: quarterly_sales\n",
      "  hash: 276aff861de54b01d75e7b8522ef1b6a6c67bfe037574fa5f9d72ab2c39a4448\n",
      "  date_created: '2025-07-21 20:32:44'\n",
      "  date_last_modified: '2025-07-21 20:32:44'\n",
      "  tag: raw\n",
      "  version: null\n",
      "  message: Initial quarterly sales data from database\n",
      "- name: quarterly_sales\n",
      "  hash: 89104c39df3a484e6b9d784f88597580058b7c746ecabbbef3fe51b6944153d8\n",
      "  date_created: '2025-07-21 20:32:44'\n",
      "  date_last_modified: '2025-07-21 20:32:44'\n",
      "  tag: enriched\n",
      "  version: null\n",
      "  message: Added revenue calculations and per-unit metrics\n",
      "- name: sales_summary\n",
      "  hash: ca232d72f83e78a169ed4ff564aa99790e5014389167e4241371a2a77c19e920\n",
      "  date_created: '2025-07-21 20:32:44'\n",
      "  date_last_modified: '2025-07-21 20:32:44'\n",
      "  tag: final\n",
      "  version: null\n",
      "  message: Final summary statistics for quarterly report\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at a collection's metadata\n",
    "with open('.datashelf/sales_analysis_2024/sales_analysis_2024_metadata.yaml', 'r') as f:\n",
    "    collection_metadata = yaml.safe_load(f)\n",
    "\n",
    "print(\"Sales Analysis 2024 collection metadata:\")\n",
    "print(yaml.dump(collection_metadata, default_flow_style=False, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this example, we've demonstrated the current capabilities of DataShelf:\n",
    "\n",
    "✅ **What DataShelf can do now:**\n",
    "- Initialize project-level dataset versioning\n",
    "- Create organized collections for related datasets\n",
    "- Save pandas DataFrames with metadata (tags, messages, timestamps)\n",
    "- Automatically detect and prevent duplicate data storage\n",
    "- Maintain comprehensive metadata about all dataset versions\n",
    "- Track dataset history with SHA-256 hashing\n",
    "\n",
    "🚧 **Coming in future versions:**\n",
    "- Load specific dataset versions by name/tag\n",
    "- Compare datasets across different time periods\n",
    "- Restore previous dataset states\n",
    "- Command-line interface\n",
    "- Support for additional data formats (Polars, etc.)\n",
    "\n",
    "DataShelf provides a solid foundation for dataset version control, making it easy to track how your data evolves throughout your analysis workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
