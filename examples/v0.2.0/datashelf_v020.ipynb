{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2687e5d8",
   "metadata": {},
   "source": [
    "# DataShelf v0.2.0: Complete Features Guide\n",
    "\n",
    "A comprehensive walkthrough of DataShelf's version control system for datasets.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction & Setup](#introduction--setup)\n",
    "2. [Core Concepts](#core-concepts)\n",
    "3. [Basic Workflow](#advanced-features)\n",
    "4. [Advanced Features](#advanced-features)\n",
    "5. [CLI Usage](#cli-usage)\n",
    "6. [Configuration Management](#configuration-management)\n",
    "7. [Data Loading & Retrieval](#data-loading--retrieval)\n",
    "8. [Metadata Inspection](#metadata-inspection)\n",
    "9. [Best Practices](#best-practices)\n",
    "10. [Troubleshooting](#troubleshooting)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5500a",
   "metadata": {},
   "source": [
    "## Introduction & Setup\n",
    "\n",
    "DataShelf is a simple version control system for datasets that helps data scientists and analysts track how their datasets evolve over time. It provides hash-based deduplication, metadata tracking, and organized collection management.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# From source\n",
    "git clone https://github.com/r0hankrishnan/datashelf.git\n",
    "df datashelf\n",
    "pip install -e.\n",
    "\n",
    "# Dev install\n",
    "pip install -e \".[dev]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030e10c",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe0a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's get started with our demo for DataShelf v0.2.0!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datashelf.core as ds\n",
    "from datashelf.core.config import check_tag_enforcement, get_allowed_tags\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducible examples\n",
    "np.random.seed(42)\n",
    "print(\"Let's get started with our demo for DataShelf v0.2.0!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a9a70",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "Before diving into the features, let's understand DataShelf's key concepts:\n",
    "- **Collections**: Logical grouping of related datasets (like folders)\n",
    "- **Versions**: Each dataset save creates a timestamped version with metadata\n",
    "- **Tags**: Labels for easy identification (\"raw\", \"cleaned\", \"final\", etc.)\n",
    "- **Messages**: Descriptive commit messages explaining dataset changes\n",
    "- **Hashing** SHA-256 based deduplication prevents storing identical data\n",
    "\n",
    "### Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bc9a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample datasets created:\n",
      "Sales data: (5, 6)\n",
      "Customer data: (10, 6)\n",
      "\n",
      "Sales data preview:\n",
      "  product_id product_name     category  units_sold  unit_price region\n",
      "0       P001     Widget A  Electronics         150       25.99  North\n",
      "1       P002     Widget B         Home         200       45.50  South\n",
      "2       P003     Widget C  Electronics          75       15.00   East\n",
      "3       P004     Widget D       Sports         300       60.00   West\n",
      "4       P005     Widget E         Home         120       35.75  North\n"
     ]
    }
   ],
   "source": [
    "# Sales data\n",
    "sales_data = pd.DataFrame({\n",
    "    'product_id': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
    "    'product_name': ['Widget A', 'Widget B', 'Widget C', 'Widget D', 'Widget E'],\n",
    "    'category': ['Electronics', 'Home', 'Electronics', 'Sports', 'Home'],\n",
    "    'units_sold': [150, 200, 75, 300, 120],\n",
    "    'unit_price': [25.99, 45.50, 15.00, 60.00, 35.75],\n",
    "    'region': ['North', 'South', 'East', 'West', 'North']\n",
    "})\n",
    "\n",
    "# Customer data - secondary dataset\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': range(1001, 1011),\n",
    "    'name': ['Alice Johnson', 'Bob Smith', 'Carol Davis', 'David Wilson', 'Eva Brown',\n",
    "             'Frank Miller', 'Grace Lee', 'Henry Taylor', 'Ivy Chen', 'Jack Robinson'],\n",
    "    'age': [28, 34, 22, 45, 31, 29, 38, 52, 26, 41],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix',\n",
    "             'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose'],\n",
    "    'total_purchases': [5, 12, 3, 8, 15, 7, 9, 4, 11, 6],\n",
    "    'avg_order_value': [45.20, 67.80, 23.50, 55.10, 89.90, 38.75, 72.30, 41.60, 58.40, 49.20]\n",
    "})\n",
    "\n",
    "print(\"Sample datasets created:\")\n",
    "print(f\"Sales data: {sales_data.shape}\")\n",
    "print(f\"Customer data: {customer_data.shape}\")\n",
    "print(\"\\nSales data preview:\")\n",
    "print(sales_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba25b97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Basic Workflow\n",
    "\n",
    "### 1. Initialize DataShelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab2f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 17:34:44,968 - INFO - .datashelf directory with config and metadata files initialized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize DataShelf in your project directory\n",
    "# This creates a .datashelf folder with config and metadata files\n",
    "ds.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b841903",
   "metadata": {},
   "source": [
    "**What happens during init():**\n",
    "- Creates `.datashelf/` directory\n",
    "- Generates `datashelf_metadata.yaml` (project-level metadata)\n",
    "- Creates `datashelf_config.yaml` (config settings)\n",
    "- Sets up tag enforcement and allowed tags\n",
    "\n",
    "### 2. Create Collections\n",
    "\n",
    "Collections help organize related datasets. Let's create collections for different types of analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd1d596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 17:34:44,983 - INFO - Collection directory: sales_analysis_q4 and metadata file created.\n",
      "2025-07-30 17:34:44,986 - INFO - Collection directory: customer_analytics and metadata file created.\n",
      "2025-07-30 17:34:44,991 - INFO - Collection directory: experimental_data and metadata file created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections created!\n"
     ]
    }
   ],
   "source": [
    "# Create collection for different analyses\n",
    "ds.create_collection(\"Sales Analysis Q4\")\n",
    "ds.create_collection(\"Customer Analytics\")\n",
    "ds.create_collection(\"Experimental Data\")\n",
    "\n",
    "print(\"Collections created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8d7be",
   "metadata": {},
   "source": [
    "### 3. Save Your First Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4acecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 17:34:45,007 - INFO - Save as CSV (0.00 MB)\n",
      "2025-07-30 17:34:45,017 - INFO - Quarterly Sales added to Sales Analysis Q4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save operation result: 0\n"
     ]
    }
   ],
   "source": [
    "result = ds.save(\n",
    "    df = sales_data,\n",
    "    collection_name = \"Sales Analysis Q4\",\n",
    "    name = \"Quarterly Sales\",\n",
    "    tag = \"raw\",\n",
    "    message = \"Initial Q4  sales data from database export\"\n",
    ")\n",
    "print(f\"Save operation result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bbe010",
   "metadata": {},
   "source": [
    "### 4. Create and Save Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62759c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 17:34:45,034 - INFO - Save as CSV (0.00 MB)\n",
      "2025-07-30 17:34:45,045 - INFO - quarterly_sales added to Sales Analysis Q4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched data saved!\n",
      "New columns: ['total_revenue', 'revenue_per_unit', 'price_category']\n"
     ]
    }
   ],
   "source": [
    "# Create enriched version with calculations\n",
    "sales_enriched = sales_data.copy()\n",
    "sales_enriched['total_revenue'] = sales_enriched['units_sold'] * sales_enriched['unit_price']\n",
    "sales_enriched['revenue_per_unit'] = sales_enriched['total_revenue'] / sales_enriched['units_sold']\n",
    "sales_enriched['price_category'] = pd.cut(\n",
    "    sales_enriched['unit_price'], \n",
    "    bins=[0, 30, 50, 100], \n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Save the enriched version\n",
    "ds.save(\n",
    "    df=sales_enriched,\n",
    "    collection_name=\"Sales Analysis Q4\",\n",
    "    name=\"quarterly_sales\",\n",
    "    tag=\"intermediate\",\n",
    "    message=\"Added revenue calculations and price categorization\"\n",
    ")\n",
    "\n",
    "print(\"Enriched data saved!\")\n",
    "print(f\"New columns: {[col for col in sales_enriched.columns if col not in sales_data.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e2526",
   "metadata": {},
   "source": [
    "### Create Summary Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f032eace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 17:34:45,067 - INFO - Save as CSV (0.00 MB)\n",
      "2025-07-30 17:34:45,080 - INFO - category_summary added to Sales Analysis Q4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data created and saved:\n",
      "      category  units_sold  total_revenue  unit_price  avg_units_per_product\n",
      "0  Electronics         225         5023.5       20.49                    NaN\n",
      "1         Home         320        13390.0       40.62                    NaN\n",
      "2       Sports         300        18000.0       60.00                    NaN\n"
     ]
    }
   ],
   "source": [
    "# Create aggregated summary\n",
    "sales_summary = sales_enriched.groupby('category').agg({\n",
    "    'units_sold': 'sum',\n",
    "    'total_revenue': 'sum',\n",
    "    'unit_price': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "sales_summary.reset_index(inplace=True)\n",
    "sales_summary['avg_units_per_product'] = sales_summary['units_sold'] / sales_enriched.groupby('category').size()\n",
    "\n",
    "# Save the summary\n",
    "ds.save(\n",
    "    df=sales_summary,\n",
    "    collection_name=\"Sales Analysis Q4\",\n",
    "    name=\"category_summary\",\n",
    "    tag=\"final\",\n",
    "    message=\"Final category-level summary for Q4 report\"\n",
    ")\n",
    "\n",
    "print(\"Summary data created and saved:\")\n",
    "print(sales_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f86048",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Advanced Features\n",
    "\n",
    "### Duplicate Detection\n",
    "\n",
    "DataShelf automatically prevents saving identical datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e92c777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 17:34:45,095 - INFO - duplicate_test's hash matches a dataframe that is already saved in datashelf: Quarterly Sales.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing duplicate detection...\n",
      "Duplicate detection result: 0\n",
      "DataShelf prevented duplicate storage!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing duplicate detection...\")\n",
    "\n",
    "# Try to save the same data again\n",
    "result = ds.save(\n",
    "    df=sales_data,  # Same data as before\n",
    "    collection_name=\"Sales Analysis Q4\",\n",
    "    name=\"duplicate_test\",\n",
    "    tag=\"raw\",\n",
    "    message=\"This should be detected as a duplicate\"\n",
    ")\n",
    "\n",
    "print(f\"Duplicate detection result: {result}\")\n",
    "print(\"DataShelf prevented duplicate storage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca763c",
   "metadata": {},
   "source": [
    "### Working with Multiple Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29700890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 17:34:45,105 - INFO - Save as CSV (0.00 MB)\n",
      "2025-07-30 17:34:45,113 - INFO - customer_profiles added to Customer Analytics\n",
      "2025-07-30 17:34:45,120 - INFO - Save as CSV (0.00 MB)\n",
      "2025-07-30 17:34:45,141 - INFO - customer_segments added to Customer Analytics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer data processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Save customer data to different collection\n",
    "ds.save(\n",
    "    df=customer_data,\n",
    "    collection_name=\"Customer Analytics\",\n",
    "    name=\"customer_profiles\",\n",
    "    tag=\"raw\",\n",
    "    message=\"Customer profile data from CRM system\"\n",
    ")\n",
    "\n",
    "# Create customer segments\n",
    "customer_segments = customer_data.copy()\n",
    "customer_segments['age_group'] = pd.cut(\n",
    "    customer_segments['age'], \n",
    "    bins=[0, 30, 40, 50, 100], \n",
    "    labels=['Young', 'Mid', 'Mature', 'Senior']\n",
    ")\n",
    "customer_segments['value_tier'] = pd.cut(\n",
    "    customer_segments['avg_order_value'], \n",
    "    bins=[0, 40, 60, 80, 100], \n",
    "    labels=['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    ")\n",
    "\n",
    "ds.save(\n",
    "    df=customer_segments,\n",
    "    collection_name=\"Customer Analytics\",\n",
    "    name=\"customer_segments\",\n",
    "    tag=\"intermediate\",\n",
    "    message=\"Added age groups and value tier segmentation\"\n",
    ")\n",
    "\n",
    "print(\"Customer data processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6689518",
   "metadata": {},
   "source": [
    "### Smart File Format Selection\n",
    "\n",
    "When saving a dataset, DataShelf automatically chooses between CSV and Parquet file formats based on data size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89a699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 17:34:45,201 - INFO - Save as Parquet (18.00 MB)\n",
      "2025-07-30 17:34:45,278 - INFO - large_dataset added to Experimental Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large dataset saved: (200000, 5)\n",
      "DataShelf automatically selected optimal file format!\n"
     ]
    }
   ],
   "source": [
    "# Create a larger dataset to demonstrate format selection\n",
    "large_data = pd.DataFrame({\n",
    "    'id': range(200000),\n",
    "    'value_1': np.random.randn(200000),\n",
    "    'value_2': np.random.randn(200000),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], 200000),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=200000, freq='H')\n",
    "})\n",
    "\n",
    "# Save large dataset (will automatically use Parquet for efficiency)\n",
    "ds.save(\n",
    "    df=large_data,\n",
    "    collection_name=\"Experimental Data\",\n",
    "    name=\"large_dataset\",\n",
    "    tag=\"raw\",\n",
    "    message=\"Large experimental dataset for format testing\"\n",
    ")\n",
    "\n",
    "print(f\"Large dataset saved: {large_data.shape}\")\n",
    "print(\"DataShelf automatically selected optimal file format!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e44360",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## CLI Usage\n",
    "\n",
    "DataShelf provides a command-line interface for common operations:\n",
    "\n",
    "### Available CLI Commands\n",
    "\n",
    "```bash\n",
    "# Initialize DataShelf\n",
    "datashelf init\n",
    "\n",
    "# Create a new collection\n",
    "datashelf create-collection \"My New Collection\"\n",
    "\n",
    "# Display DataShelf metadata\n",
    "datashelf ls ds-md\n",
    "\n",
    "# Display collections overview\n",
    "datashelf ls ds-coll\n",
    "\n",
    "# Display collection metadata (will prompt for collection name)\n",
    "datashelf ls coll-md\n",
    "\n",
    "# Display collection files (will prompt for collection name)\n",
    "datashelf checkout collection_name hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8fc59d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI Examples:\n",
      "==================================================\n",
      "$ datashelf init\n",
      "\n",
      "# Create a collection\n",
      "$ datashelf create-collection 'sales Analysis Q4'\n",
      "\n",
      "# View collections\n",
      "$ datashelf ls ds-coll\n",
      "\n",
      "# View collection files\n",
      "$ datashelf ls coll-files\n",
      "Collection name? Sales Analysis Q4\n",
      "\n",
      "# Checkout a specific dataset\n",
      "$ datashelf checkout 'Sales Analysis Q4' abc123...\n"
     ]
    }
   ],
   "source": [
    "# Simulate CLI commands (these would be run in the terminal)\n",
    "print(\"CLI Examples:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"$ datashelf init\")\n",
    "print()\n",
    "print(\"# Create a collection\")\n",
    "print(\"$ datashelf create-collection 'sales Analysis Q4'\")\n",
    "print()\n",
    "print(\"# View collections\")\n",
    "print(\"$ datashelf ls ds-coll\")\n",
    "print()\n",
    "print(\"# View collection files\")\n",
    "print(\"$ datashelf ls coll-files\")\n",
    "print(\"Collection name? Sales Analysis Q4\")\n",
    "print()\n",
    "print(\"# Checkout a specific dataset\")\n",
    "print(\"$ datashelf checkout 'Sales Analysis Q4' abc123...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e391a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Configuration Management\n",
    "\n",
    "### Tag Enforcement\n",
    "\n",
    "DataShelf comes with a set of allowable tags out-of-the-box. While there are functions available to disable tag enforcement, **disabling tag enforcement is not recommended**. Having a set of allowed tags makes organizing and viewing saved datasets much more straight-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e43382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 17:34:45,290 - ERROR - invalid_tag is not a valid tag.Tag enforcement is currently set to True.You cannot change enforcement as of this version of DataShelf.Please use one of the following allowed tags raw, intermediate, cleaned, ad-hoc, final\n",
      "2025-07-30 17:34:45,296 - INFO - Save as CSV (0.00 MB)\n",
      "2025-07-30 17:34:45,307 - INFO - test_data added to Experimental Data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag enforcement enabled: True\n",
      "Allowed tags: ['raw', 'intermediate', 'cleaned', 'ad-hoc', 'final']\n",
      "Tag validation caught invalid tag: invalid_tag is not a valid tag.Tag enforcement is currently set to True.You cannot change enforcement as of this version of DataShelf.Please use one of the following allowed tags raw, intermediate, cleaned, ad-hoc, final\n",
      "Valid tag accepted!\n"
     ]
    }
   ],
   "source": [
    "# Check current tag enforcement status\n",
    "print(f\"Tag enforcement enabled: {check_tag_enforcement()}\")\n",
    "print(f\"Allowed tags: {get_allowed_tags()}\")\n",
    "\n",
    "# Demonstrate tag validation\n",
    "try:\n",
    "    ds.save(\n",
    "        df=sales_data.head(2),\n",
    "        collection_name=\"Experimental Data\",\n",
    "        name=\"test_data\",\n",
    "        tag=\"invalid_tag\",  # This will fail\n",
    "        message=\"Testing tag validation\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Tag validation caught invalid tag: {e}\")\n",
    "\n",
    "# Use a valid tag\n",
    "ds.save(\n",
    "    df=sales_data.head(2),\n",
    "    collection_name=\"Experimental Data\",\n",
    "    name=\"test_data\",\n",
    "    tag=\"ad-hoc\",  # This is valid\n",
    "    message=\"Testing with valid ad-hoc tag\"\n",
    ")\n",
    "print(\"Valid tag accepted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082efa60",
   "metadata": {},
   "source": [
    "### Configuration Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9532161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default DataShelf Configuration:\n",
      "- tag_enforcement: True\n",
      "- allowed_tags: ['raw', 'intermediate', 'cleaned', 'ad-hoc', 'final']\n",
      "- collection_tag_overrides: {} (for future use)\n"
     ]
    }
   ],
   "source": [
    "# The default configuration includes:\n",
    "print(\"Default DataShelf Configuration:\")\n",
    "print(\"- tag_enforcement: True\")\n",
    "print(\"- allowed_tags: ['raw', 'intermediate', 'cleaned', 'ad-hoc', 'final']\")\n",
    "print(\"- collection_tag_overrides: {} (for future use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b580c5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Data Loading & Retrieval\n",
    "\n",
    "### Load Datasets Back into Memory\n",
    "\n",
    "The `load()` function retrieves a specific dataset version and returns it as a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ab1fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━┓\n",
       "┃<span style=\"color: #a5b4fc; text-decoration-color: #a5b4fc; font-weight: bold\"> na… </span>┃<span style=\"color: #a5b4fc; text-decoration-color: #a5b4fc; font-weight: bold\"> hash                                                             </span>┃<span style=\"color: #a5b4fc; text-decoration-color: #a5b4fc; font-weight: bold\"> da… </span>┃<span style=\"color: #a5b4fc; text-decoration-color: #a5b4fc; font-weight: bold\"> da… </span>┃<span style=\"color: #a5b4fc; text-decoration-color: #a5b4fc; font-weight: bold\"> tag </span>┃<span style=\"color: #a5b4fc; text-decoration-color: #a5b4fc; font-weight: bold\"> ve… </span>┃<span style=\"color: #a5b4fc; text-decoration-color: #a5b4fc; font-weight: bold\"> me… </span>┃<span style=\"color: #a5b4fc; text-decoration-color: #a5b4fc; font-weight: bold\"> fi… </span>┃<span style=\"color: #a5b4fc; text-decoration-color: #a5b4fc; font-weight: bold\"> d… </span>┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━┩\n",
       "│ sa… │                                                                  │ 20… │     │     │     │     │ /U… │ F… │\n",
       "│     │                                                                  │ 17… │     │     │     │     │ am… │    │\n",
       "│     │                                                                  │     │     │     │     │     │ na… │    │\n",
       "├─────┼──────────────────────────────────────────────────────────────────┼─────┼─────┼─────┼─────┼─────┼─────┼────┤\n",
       "│ Qu… │ 9d77eabf6b934ce8e759742429021d0afeb2ccaa339c2db35ea4c96fdf96ff3f │ 20… │ 20… │ raw │ 1   │ In… │ /U… │ F… │\n",
       "│ Sa… │                                                                  │ 17… │ 17… │     │     │ Q4  │ am… │    │\n",
       "│     │                                                                  │     │     │     │     │ sa… │ ly… │    │\n",
       "│     │                                                                  │     │     │     │     │ da… │     │    │\n",
       "│     │                                                                  │     │     │     │     │ fr… │     │    │\n",
       "│     │                                                                  │     │     │     │     │ da… │     │    │\n",
       "│     │                                                                  │     │     │     │     │ ex… │     │    │\n",
       "├─────┼──────────────────────────────────────────────────────────────────┼─────┼─────┼─────┼─────┼─────┼─────┼────┤\n",
       "│ qu… │ 3976e38c4a19642f63ba16bb786b1eefe40313040e2e907830a4dc4205d12a10 │ 20… │ 20… │ in… │ 2   │ Ad… │ /U… │ F… │\n",
       "│     │                                                                  │ 17… │ 17… │     │     │ re… │ am… │    │\n",
       "│     │                                                                  │     │     │     │     │ ca… │ ly… │    │\n",
       "│     │                                                                  │     │     │     │     │ and │     │    │\n",
       "│     │                                                                  │     │     │     │     │ pr… │     │    │\n",
       "│     │                                                                  │     │     │     │     │ ca… │     │    │\n",
       "├─────┼──────────────────────────────────────────────────────────────────┼─────┼─────┼─────┼─────┼─────┼─────┼────┤\n",
       "│ ca… │ 887054aa634c5fc4fc6b4c9e097f29d69d252902c42099832d8497a85aae333f │ 20… │ 20… │ fi… │ 3   │ Fi… │ /U… │ F… │\n",
       "│     │                                                                  │ 17… │ 17… │     │     │ ca… │ am… │    │\n",
       "│     │                                                                  │     │     │     │     │ su… │ y_… │    │\n",
       "│     │                                                                  │     │     │     │     │ for │     │    │\n",
       "│     │                                                                  │     │     │     │     │ Q4  │     │    │\n",
       "│     │                                                                  │     │     │     │     │ re… │     │    │\n",
       "└─────┴──────────────────────────────────────────────────────────────────┴─────┴─────┴─────┴─────┴─────┴─────┴────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━┓\n",
       "┃\u001b[1;38;2;165;180;252m \u001b[0m\u001b[1;38;2;165;180;252mna…\u001b[0m\u001b[1;38;2;165;180;252m \u001b[0m┃\u001b[1;38;2;165;180;252m \u001b[0m\u001b[1;38;2;165;180;252mhash                                                            \u001b[0m\u001b[1;38;2;165;180;252m \u001b[0m┃\u001b[1;38;2;165;180;252m \u001b[0m\u001b[1;38;2;165;180;252mda…\u001b[0m\u001b[1;38;2;165;180;252m \u001b[0m┃\u001b[1;38;2;165;180;252m \u001b[0m\u001b[1;38;2;165;180;252mda…\u001b[0m\u001b[1;38;2;165;180;252m \u001b[0m┃\u001b[1;38;2;165;180;252m \u001b[0m\u001b[1;38;2;165;180;252mtag\u001b[0m\u001b[1;38;2;165;180;252m \u001b[0m┃\u001b[1;38;2;165;180;252m \u001b[0m\u001b[1;38;2;165;180;252mve…\u001b[0m\u001b[1;38;2;165;180;252m \u001b[0m┃\u001b[1;38;2;165;180;252m \u001b[0m\u001b[1;38;2;165;180;252mme…\u001b[0m\u001b[1;38;2;165;180;252m \u001b[0m┃\u001b[1;38;2;165;180;252m \u001b[0m\u001b[1;38;2;165;180;252mfi…\u001b[0m\u001b[1;38;2;165;180;252m \u001b[0m┃\u001b[1;38;2;165;180;252m \u001b[0m\u001b[1;38;2;165;180;252md…\u001b[0m\u001b[1;38;2;165;180;252m \u001b[0m┃\n",
       "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━┩\n",
       "│ sa… │                                                                  │ 20… │     │     │     │     │ /U… │ F… │\n",
       "│     │                                                                  │ 17… │     │     │     │     │ am… │    │\n",
       "│     │                                                                  │     │     │     │     │     │ na… │    │\n",
       "├─────┼──────────────────────────────────────────────────────────────────┼─────┼─────┼─────┼─────┼─────┼─────┼────┤\n",
       "│ Qu… │ 9d77eabf6b934ce8e759742429021d0afeb2ccaa339c2db35ea4c96fdf96ff3f │ 20… │ 20… │ raw │ 1   │ In… │ /U… │ F… │\n",
       "│ Sa… │                                                                  │ 17… │ 17… │     │     │ Q4  │ am… │    │\n",
       "│     │                                                                  │     │     │     │     │ sa… │ ly… │    │\n",
       "│     │                                                                  │     │     │     │     │ da… │     │    │\n",
       "│     │                                                                  │     │     │     │     │ fr… │     │    │\n",
       "│     │                                                                  │     │     │     │     │ da… │     │    │\n",
       "│     │                                                                  │     │     │     │     │ ex… │     │    │\n",
       "├─────┼──────────────────────────────────────────────────────────────────┼─────┼─────┼─────┼─────┼─────┼─────┼────┤\n",
       "│ qu… │ 3976e38c4a19642f63ba16bb786b1eefe40313040e2e907830a4dc4205d12a10 │ 20… │ 20… │ in… │ 2   │ Ad… │ /U… │ F… │\n",
       "│     │                                                                  │ 17… │ 17… │     │     │ re… │ am… │    │\n",
       "│     │                                                                  │     │     │     │     │ ca… │ ly… │    │\n",
       "│     │                                                                  │     │     │     │     │ and │     │    │\n",
       "│     │                                                                  │     │     │     │     │ pr… │     │    │\n",
       "│     │                                                                  │     │     │     │     │ ca… │     │    │\n",
       "├─────┼──────────────────────────────────────────────────────────────────┼─────┼─────┼─────┼─────┼─────┼─────┼────┤\n",
       "│ ca… │ 887054aa634c5fc4fc6b4c9e097f29d69d252902c42099832d8497a85aae333f │ 20… │ 20… │ fi… │ 3   │ Fi… │ /U… │ F… │\n",
       "│     │                                                                  │ 17… │ 17… │     │     │ ca… │ am… │    │\n",
       "│     │                                                                  │     │     │     │     │ su… │ y_… │    │\n",
       "│     │                                                                  │     │     │     │     │ for │     │    │\n",
       "│     │                                                                  │     │     │     │     │ Q4  │     │    │\n",
       "│     │                                                                  │     │     │     │     │ re… │     │    │\n",
       "└─────┴──────────────────────────────────────────────────────────────────┴─────┴─────┴─────┴─────┴─────┴─────┴────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First, let's get some hash values to work with\n",
    "ds.ls(\"coll-files\")  # This will prompt for collection name: \"Sales Analysis Q4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36ac8329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "  product_id product_name     category  units_sold  unit_price region\n",
      "0       P001     Widget A  Electronics         150       25.99  North\n",
      "1       P002     Widget B         Home         200       45.50  South\n",
      "2       P003     Widget C  Electronics          75       15.00   East\n",
      "3       P004     Widget D       Sports         300       60.00   West\n",
      "4       P005     Widget E         Home         120       35.75  North\n"
     ]
    }
   ],
   "source": [
    "# Load a specific dataset version using its hash\n",
    "# Note: Replace 'hash_value' with actual hash from your metadata\n",
    "try:\n",
    "    # This is a demonstration - you'd use actual hash values from your metadata\n",
    "    loaded_data = ds.load(\"Sales Analysis Q4\", \"9d77eabf6b934ce8e759742429021d0afeb2ccaa339c2db35ea4c96fdf96ff3f\")\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(loaded_data.head())\n",
    "except:\n",
    "    print(\"Demo: Use actual hash values from your collection metadata to load data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec87c0",
   "metadata": {},
   "source": [
    "### Checkout Datasets to Files\n",
    "\n",
    "The `checkout()` function retrieves a specific dataset and copies to the same directory that .datashelf/ is in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da4b6cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo: checkout() copies dataset files to your working directory\n",
      "Use: ds.checkout('collection_name', 'hash_value')\n"
     ]
    }
   ],
   "source": [
    "# Checkout copies a dataset file to your working directory\n",
    "# ds.checkout(\"Sales Analysis Q4\", \"your_actual_hash_here\")\n",
    "print(\"Demo: checkout() copies dataset files to your working directory\")\n",
    "print(\"Use: ds.checkout('collection_name', 'hash_value')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db5958",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Metadata Inspection\n",
    "\n",
    "DataShelf provides comprehensive metadata viewing capabilities:\n",
    "\n",
    "### Using the Display Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c72c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataShelf creates the following structure:\n",
      "\n",
      "your_project/\n",
      "├── .datashelf/\n",
      "│   ├── datashelf_metadata.yaml      # Project-level metadata\n",
      "│   ├── datashelf_config.yaml        # Configuration settings\n",
      "│   └── collection_name/\n",
      "│       ├── collection_metadata.yaml # Collection-specific metadata  \n",
      "│       └── dataset_files.[csv|parquet] # Your actual datasets\n",
      "├── your_notebooks.ipynb\n",
      "└── your_scripts.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"DataShelf creates the following structure:\")\n",
    "print(\"\"\"\n",
    "your_project/\n",
    "├── .datashelf/\n",
    "│   ├── datashelf_metadata.yaml      # Project-level metadata\n",
    "│   ├── datashelf_config.yaml        # Configuration settings\n",
    "│   └── collection_name/\n",
    "│       ├── collection_metadata.yaml # Collection-specific metadata  \n",
    "│       └── dataset_files.[csv|parquet] # Your actual datasets\n",
    "├── your_notebooks.ipynb\n",
    "└── your_scripts.py\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc999ef",
   "metadata": {},
   "source": [
    "### Metadata Fields Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "febd09bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Metadata Fields:\n",
      "==============================\n",
      "\n",
      "Project Level (datashelf_metadata.yaml):\n",
      "- date_created: When DataShelf was initialized\n",
      "- number_of_collections: Total collections count\n",
      "- collections: List of all collections with their metadata\n",
      "\n",
      "Collection Level (collection_metadata.yaml): \n",
      "- collection_name: Name of the collection\n",
      "- date_created/date_last_modified: Timestamps\n",
      "- number_of_files: Count of datasets in collection\n",
      "- most_recent_commit: Path to latest saved dataset\n",
      "- max_version: Highest version number used\n",
      "\n",
      "File Level (within collections):\n",
      "- name: Dataset name\n",
      "- hash: SHA-256 hash for deduplication\n",
      "- tag: User-assigned tag (raw, cleaned, etc.)\n",
      "- version: Auto-incrementing version number\n",
      "- message: Commit message\n",
      "- file_path: Full path to dataset file\n",
      "- deleted: Soft deletion flag (future feature)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Key Metadata Fields:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"\"\"\n",
    "Project Level (datashelf_metadata.yaml):\n",
    "- date_created: When DataShelf was initialized\n",
    "- number_of_collections: Total collections count\n",
    "- collections: List of all collections with their metadata\n",
    "\n",
    "Collection Level (collection_metadata.yaml): \n",
    "- collection_name: Name of the collection\n",
    "- date_created/date_last_modified: Timestamps\n",
    "- number_of_files: Count of datasets in collection\n",
    "- most_recent_commit: Path to latest saved dataset\n",
    "- max_version: Highest version number used\n",
    "\n",
    "File Level (within collections):\n",
    "- name: Dataset name\n",
    "- hash: SHA-256 hash for deduplication\n",
    "- tag: User-assigned tag (raw, cleaned, etc.)\n",
    "- version: Auto-incrementing version number\n",
    "- message: Commit message\n",
    "- file_path: Full path to dataset file\n",
    "- deleted: Soft deletion flag (future feature)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8720f96",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Project Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9d0e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Project Organization Best Practices:\n",
      "\n",
      "1. Initialize DataShelf in your project root directory\n",
      "2. Create meaningful collection names:\n",
      "   ✓ \"Customer_Analytics_2024\"\n",
      "   ✓ \"Sales_Forecasting_Models\" \n",
      "   ✖ \"data\" or \"temp\"\n",
      "\n",
      "3. Use consistent naming conventions:\n",
      "   - snake_case for collection names\n",
      "   - descriptive dataset names\n",
      "   - standardized tags\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"📁 Project Organization Best Practices:\")\n",
    "print(\"\"\"\n",
    "1. Initialize DataShelf in your project root directory\n",
    "2. Create meaningful collection names:\n",
    "   \\u2713 \"Customer_Analytics_2024\"\n",
    "   \\u2713 \"Sales_Forecasting_Models\" \n",
    "   \\u2716 \"data\" or \"temp\"\n",
    "\n",
    "3. Use consistent naming conventions:\n",
    "   - snake_case for collection names\n",
    "   - descriptive dataset names\n",
    "   - standardized tags\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ec73e",
   "metadata": {},
   "source": [
    "### 2. Tagging Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c1b4a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Tagging Strategy:\n",
      "\n",
      "- raw: Original, unmodified data from source\n",
      "- intermediate: Partially processed, work-in-progress\n",
      "- cleaned: Cleaned and validated data\n",
      "- ad-hoc: Experimental or one-off analysis\n",
      "- final: Completed, ready-for-use datasets\n",
      "\n",
      "Custom workflow example:\n",
      "raw → intermediate → cleaned → final\n",
      "  ↓\n",
      "ad-hoc (for experiments)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Recommended Tagging Strategy:\")\n",
    "print(\"\"\"\n",
    "- raw: Original, unmodified data from source\n",
    "- intermediate: Partially processed, work-in-progress\n",
    "- cleaned: Cleaned and validated data\n",
    "- ad-hoc: Experimental or one-off analysis\n",
    "- final: Completed, ready-for-use datasets\n",
    "\n",
    "Custom workflow example:\n",
    "raw → intermediate → cleaned → final\n",
    "  ↓\n",
    "ad-hoc (for experiments)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8959461",
   "metadata": {},
   "source": [
    "### 3. Commit Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46ce039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective Commit Messages:\n",
      "\n",
      "✓ Good examples:\n",
      "- \"Added revenue calculations and price tiers\"\n",
      "- \"Removed outliers and standardized categories\"\n",
      "- \"Final aggregation for Q4 executive report\"\n",
      "\n",
      "✖ Avoid:\n",
      "- \"updated data\"\n",
      "- \"fixes\"\n",
      "- \"version 2\"\n",
      "\n",
      "Be specific about what changed and why!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Effective Commit Messages:\")\n",
    "print(\"\"\"\n",
    "\\u2713 Good examples:\n",
    "- \"Added revenue calculations and price tiers\"\n",
    "- \"Removed outliers and standardized categories\"\n",
    "- \"Final aggregation for Q4 executive report\"\n",
    "\n",
    "\\u2716 Avoid:\n",
    "- \"updated data\"\n",
    "- \"fixes\"\n",
    "- \"version 2\"\n",
    "\n",
    "Be specific about what changed and why!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e69dc55",
   "metadata": {},
   "source": [
    "### 4. Version Control Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd0b0aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Workflow:\n",
      "\n",
      "1. Save raw data immediately upon import\n",
      "2. Document each transformation step\n",
      "3. Use meaningful tags to track processing stages\n",
      "4. Save intermediate versions of complex transformations\n",
      "5. Tag final outputs clearly for easy identification\n",
      "6. Leverage duplicate detection to avoid waste\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Recommended Workflow:\")\n",
    "print(\"\"\"\n",
    "1. Save raw data immediately upon import\n",
    "2. Document each transformation step\n",
    "3. Use meaningful tags to track processing stages\n",
    "4. Save intermediate versions of complex transformations\n",
    "5. Tag final outputs clearly for easy identification\n",
    "6. Leverage duplicate detection to avoid waste\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359feea3",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0596532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ Troubleshooting Guide:\n",
      "==============================\n",
      "\n",
      "Issue: \"NotADirectoryError: .datashelf does not exist\"\n",
      "Solution: Run ds.init() first to initialize DataShelf\n",
      "\n",
      "Issue: \"Tag validation error\"\n",
      "Solution: Check allowed tags with get_allowed_tags() and use valid tags\n",
      "\n",
      "Issue: \"Collection already exists\"  \n",
      "Solution: This is normal - DataShelf won't overwrite existing collections\n",
      "\n",
      "Issue: \"Cannot find .datashelf directory\"\n",
      "Solution: Make sure you're in the correct directory or that DataShelf was initialized\n",
      "\n",
      "Issue: File format questions\n",
      "Solution: DataShelf auto-selects CSV (<10MB) or Parquet (≥10MB) for optimal performance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\u2699 Troubleshooting Guide:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"\"\"\n",
    "Issue: \"NotADirectoryError: .datashelf does not exist\"\n",
    "Solution: Run ds.init() first to initialize DataShelf\n",
    "\n",
    "Issue: \"Tag validation error\"\n",
    "Solution: Check allowed tags with get_allowed_tags() and use valid tags\n",
    "\n",
    "Issue: \"Collection already exists\"  \n",
    "Solution: This is normal - DataShelf won't overwrite existing collections\n",
    "\n",
    "Issue: \"Cannot find .datashelf directory\"\n",
    "Solution: Make sure you're in the correct directory or that DataShelf was initialized\n",
    "\n",
    "Issue: File format questions\n",
    "Solution: DataShelf auto-selects CSV (<10MB) or Parquet (≥10MB) for optimal performance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16740495",
   "metadata": {},
   "source": [
    "### Checking DataShelf Status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a89dcb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataShelf initialized\n",
      "✓ Found 3 collections\n",
      "✓ Configuration file present\n"
     ]
    }
   ],
   "source": [
    "# Quick health check function\n",
    "def datashelf_status():\n",
    "    try:\n",
    "        from pathlib import Path\n",
    "        datashelf_path = Path.cwd() / '.datashelf'\n",
    "        if datashelf_path.exists():\n",
    "            print(\"\\u2713 DataShelf initialized\")\n",
    "            collections = [p for p in datashelf_path.iterdir() if p.is_dir()]\n",
    "            print(f\"\\u2713 Found {len(collections)} collections\")\n",
    "            \n",
    "            # Check config\n",
    "            config_path = datashelf_path / 'datashelf_config.yaml'\n",
    "            if config_path.exists():\n",
    "                print(\"\\u2713 Configuration file present\")\n",
    "            else:\n",
    "                print(\"\\u26A0  Configuration file missing\")\n",
    "                \n",
    "        else:\n",
    "            print(\"\\u2716 DataShelf not initialized - run ds.init()\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\u2716 Error checking status: {e}\")\n",
    "\n",
    "datashelf_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127e248",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "DataShelf v0.2.0 provides a robust foundation for dataset version control with the following capabilities:\n",
    "\n",
    "### Current Features\n",
    "- **Project initialization** with `ds.init()`\n",
    "- **Collection management** with `ds.create_collection()`\n",
    "- **Dataset versioning** with `ds.save()`\n",
    "- **Duplicate detection** via SHA-256 hashing\n",
    "- **Smart file format selection** (CSV/Parquet)\n",
    "- **Configurable tag enforcement**\n",
    "- **Comprehensive metadata tracking**\n",
    "- **CLI interface** for common operations\n",
    "- **Data loading** with `ds.load()` and `ds.checkout()`\n",
    "- **Metadata inspection** with `ds.ls()`\n",
    "\n",
    "### Future Enhancements\n",
    "- Advanced query capabilities\n",
    "- Data comparison tools\n",
    "- Branch-like functionality\n",
    "- Integration with popular ML frameworks\n",
    "- Enhanced CLI features\n",
    "- Support for additional data formats\n",
    "\n",
    "### Getting Started Checklist\n",
    "1. Install DataShelf: `pip install -e .`\n",
    "2. Initialize project: `ds.init()`\n",
    "3. Create collections: `ds.create_collection(\"My Analysis\")`\n",
    "4. Save datasets: `ds.save(df, collection_name, name, tag, message)`\n",
    "5. Explore metadata: `ds.ls(\"ds-md\")`\n",
    "6. Load previous versions: `ds.load(collection_name, hash_value)`\n",
    "\n",
    "DataShelf helps you maintain organized, traceable, and efficient dataset workflows.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
